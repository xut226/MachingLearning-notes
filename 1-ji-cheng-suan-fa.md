一个机器学习想要取得好的效果，需要满足两个条件：

1. 模型在训练集上的表现要足够好：training error足够小
2. 模型的VC-dimension要低，即模型的自由度不能太大，否则容易overfit。

选择何种算法，主要依据是“模型的可控性”

1. 对于 LR 这样的模型。如果 underfit，我们可以通过加 feature，或者通过高次的特征转换来使得我们的模型在训练数据上取得足够高的正确率。而对于 tree-enseble 来说，我们解决这一问题的方法是通过训练更多的 “弱弱” 的 tree. 所以，这两类模型都可以把 training error 做的足够低，也就是说模型的表达能力都是足够的。

2. 在 tree-ensemble 模型中，通过加 tree 的方式，对于模型的 vc-dimension 的改变是比较小的。而在 LR 中，初始的维数设定，或者说特征的高次转换对于 vc-dimension 的影响都是更大的。所以，一不小心我们设定的多项式维数高了，模型就 “刹不住车了”。这也就是我们之前说的，tree-ensemble 模型的可控性更好，也即更不容易 overfit.



