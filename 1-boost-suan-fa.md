一个机器学习想要取得好的效果，需要满足两个条件：

1. 模型在训练集上的表现要足够好：training error足够小
2. 模型的VC-dimension要低，即模型的自由度不能太大，否则容易overfit。

选择何种算法，主要依据是“模型的可控性”

1. 对于 LR 这样的模型。如果 underfit，我们可以通过加 feature，或者通过高次的特征转换来使得我们的模型在训练数据上取得足够高的正确率。而对于 tree-enseble 来说，我们解决这一问题的方法是通过训练更多的 “弱弱” 的 tree. 所以，这两类模型都可以把 training error 做的足够低，也就是说模型的表达能力都是足够的。

2. 在 tree-ensemble 模型中，通过加 tree 的方式，对于模型的 vc-dimension 的改变是比较小的。而在 LR 中，初始的维数设定，或者说特征的高次转换对于 vc-dimension 的影响都是更大的。所以，一不小心我们设定的多项式维数高了，模型就 “刹不住车了”。这也就是我们之前说的，tree-ensemble 模型的可控性更好，也即更不容易 overfit.


提升（Boost）简单地来说，提升就是指每一步都产生一个弱预测模型，然后加权累加到总模型中，然后每一步弱预测模型生成的的依据都是损失函数的负梯度方向，这样若干步以后就可以达到逼近损失函数局部最小值的目标。

Boost是一个加法模型，由若干个基函数及其权值乘积之和的累加
    $$f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)$$

其中$$b$$是基函数，$$\beta$$是基函数的系数，这就是我们最终分类器的样子，现在的目标就是想办法使损失函数的期望取最小值，也就是
$$min_{\beta_m,\gamma_m}\sum_{i=1}^NL(y_i,\sum_{m=1}^M\beta_mb(x;\gamma_m))$$
其中$$N$$是迭代步数。
对这M个分类器同时实行优化问题太过复杂，因此每一步只对其中一个基函数及其系数进行求解，这样逐步逼近损失函数的最小值，也就是：
    $$min_{\beta_m,\gamma_m}\sum_{i=1}^NL(y_i,f_{m-1}}+beta_mb(x;\gamma_m))$$
使损失函数最小，那就得使新加的这一项刚好等于损失函数的负梯度
    $$beta_mb(x;\gamma_m)=-\lambda\frac{\partial L(y,f_{m-1})}{\partial f}$$
