# 1.决策树 Decision Tree

一堆决策结构以树型方式组合，叶子节点代表最终的预测值或类别。遵循分而治之“策略”。  
决策树分为回归树和分类树

* 回归树预测实际值
* 分类树预测分类标签值  
  常见的DT有：

* [ ] ID3

* [ ] C4.5

* [ ] CART

## 1.1 **ID3**

采用信息增益度量划分质量。

* 信息熵

样本类别集合：D={d1,d2,...,dn},样本概率分别为：

$$P={p1,p2,...pn}，sub：p1+p2+...+pn=1$$

信息熵为：

$$Ent(x) = -\sum_{i=0}^n{pi * log(pi)}$$

* 信息增益

假定在离散量属性$$a$$上有$$V$$个可能的取值，若使用a对样本集合D划分，则产生$$V $$个分支，记$$Dv$$为$$a$$值为$$v$$的集合，信息增益为：

$$Dv = Ent(D) - \sum_{v=1}^V{\frac{Dv}{D} * Ent(Dv)}$$

一般而言，信息增益越大，代表使用属性$$a$$进行划分获得的“纯度提升越大”

使用信息增益最大的先划分。

* 优缺点

  缺点：信息增益准则对可取数目较多的属性有所偏好，带来不利影响，C4.5使用“增益率”划分最优属性。

## 1.2 **C4.5**

C4.5使用增益率选择最优划分准则

* 增益率

$$Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$  
其中  $$IV(a) = -\sum_{v=1}^V{\frac{|D^v|}{|D|}log_2{\frac{|D^v|}{|D|}}}$$

称为属性$$a$$的“固有值“，$$a$$的可能取值数目越多，则$$IV(a)$$的值通常会越大。

* 优缺点

  增益率准则对可取数目较少的属性有所偏好，因此，C4.5并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找到信息增益高于平均水平的属性，再从中选择增益率最高的。

## 1.3 CART

* 基尼指数


$$
  \begin{equation}\begin{split}
  Gini(D)& = \sum_{k=1}^{|y|} \sum_{k'}p_kp_k' \\
  &= 1 - \sum_{k=1}^{|y|}p_k^2
  \end{split}\end{equation}
$$


$$Gini(D)$$反映了从数据集$$D$$中随机抽取两个样本，其类别标记不一致的概率。因此，$$Gini(D)$$越小，数据集$$D$$的纯度越高。

# GBDT\(Gradient Boosting Decision Tree\)

又称 Multiple Additive Regression Tree,GBDT中的树都是回归树  
GBDT算法基树采用CART回归树，树节点的划分指标是平方损失函数，叶子节点的值是落在该叶子节点所有样本的目标均值。树与树之间的Boosting逻辑是：新树拟合的目标是上一课树的损失函数的负梯度的值。GBDT最终的输出结果是将样本在所有树上的叶子值相加。

## GBDT工作原理

举例：  
        人的性别判别/年龄预测，每个instance都是一个已知性别/年龄的人，而feature则包括这个人上网的时长、上网的时段、网购所花的金额等。

传统分类决策树训练过程：

![](/assets/1.1.1 DT_train.png)

GBDT训练过程：

![](/assets/1.1.1GBDT_train.png)

## GBDT的正则化

主要有以下措施：  
1，Early Stopping，本质是在某项指标达标后就停止训练，也就是设定了训练的轮数；  
2，Shrinkage，其实就是学习率，具体做法是将没课树的效果进行缩减，比如说将每棵树的结果乘以0.1，也就是降低单棵树的决策权重，相信集体决策；  
3，Subsampling，无放回抽样，具体含义是每轮训练随机使用部分训练样本，其实这里是借鉴了随机森林的思想；  
4，Dropout，这个方法是论文里的方法，没有在SKLearn中看到实现，做法是每棵树拟合的不是之前全部树ensemble后的残差，而是随机挑选一些树的残差，这个效果如何有待商榷。

