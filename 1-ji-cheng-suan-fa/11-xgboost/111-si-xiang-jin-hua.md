1.决策树 Decision Tree

一堆决策结构以树型方式组合，叶子节点代表最终的预测值或类别。遵循分而治之“策略”。

常见的DT有：

* [ ] ID3
* [ ] C4.5
* [ ] CART

1.1 **ID3**

采用信息增益度量划分质量。

* 信息熵

样本类别集合：D={d1,d2,...,dn},样本概率分别为：

$$P={p1,p2,...pn}，sub：p1+p2+...+pn=1$$

信息熵为：

$$Ent(x) = -\sum_{i=0}^n{pi * log(pi)}$$

* 信息增益

假定在离散量属性$$a$$上有$$V$$个可能的取值，若使用a对样本集合D划分，则产生$$V $$个分支，记$$Dv$$为$$a$$值为$$v$$的集合，信息增益为：

$$Dv = Ent(D) - \sum_{v=1}^V{\frac{Dv}{D} * Ent(Dv)}$$

一般而言，信息增益越大，代表使用属性$$a$$进行划分获得的“纯度提升越大”

使用信息增益最大的先划分。

* 优缺点

  缺点：信息增益准则对可取数目较多的属性有所偏好，带来不利影响，C4.5使用“增益率”划分最优属性。

1.2 **C4.5**  
    C4.5使用增益率选择最优划分准则

* 增益率

$$Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$  
其中  
$$IV(a) = -\sum_{v=1}^V{\frac{|D^v|}{|D|}log_2{\frac{|D^v|}{|D|}}}$$

称为属性$$a$$的“固有值“，$$a$$的可能取值数目越多，则$$IV(a)$$的值通常会越大。

* 优缺点

    增益率准则对可取数目较少的属性有所偏好，因此，C4.5并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找到信息增益高于平均水平的属性，再从中选择增益率最高的。

1.3 CART
* 基尼指数

$$Gini(D) = \sum_{k=1}^{|y|} \sum_{k^'}$$