# GBDT\(Gradient Boosting Decision Tree\)

又称 Multiple Additive Regression Tree,GBDT中的树都是回归树  
        GBDT算法基树采用CART回归树，树节点的划分指标是平方损失函数，叶子节点的值是落在该叶子节点所有样本的目标均值。树与树之间的Boosting逻辑是：新树拟合的目标是上一课树的损失函数的负梯度的值。GBDT最终的输出结果是将样本在所有树上的叶子值相加。



## GBDT的正则化

主要有以下措施：  
1，Early Stopping，本质是在某项指标达标后就停止训练，也就是设定了训练的轮数；  
2，Shrinkage，其实就是学习率，具体做法是将没课树的效果进行缩减，比如说将每棵树的结果乘以0.1，也就是降低单棵树的决策权重，相信集体决策；  
3，Subsampling，无放回抽样，具体含义是每轮训练随机使用部分训练样本，其实这里是借鉴了随机森林的思想；  
4，Dropout，这个方法是论文里的方法，没有在SKLearn中看到实现，做法是每棵树拟合的不是之前全部树ensemble后的残差，而是随机挑选一些树的残差，这个效果如何有待商榷。

